{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import groupby\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_output_file(filename):\n",
    "    return os.path.join(os.getcwd(), 'output_data', filename)\n",
    "\n",
    "def import_data():\n",
    "    return pd.read_csv(get_output_file('01_data_extraction_final.csv'), index_col=0)\n",
    "\n",
    "def import_ner_data():\n",
    "    return pd.read_csv(get_output_file('02_data_preprocessing_ner.csv'), index_col=0)\n",
    "\n",
    "def import_dictionary_data():\n",
    "    return pd.read_csv(get_output_file('02_data_preprocessing_dictionary.csv'), index_col=0)\n",
    "\n",
    "def export_ner_data(df):\n",
    "    df.to_csv(get_output_file('02_data_preprocessing_ner.csv'))\n",
    "    \n",
    "def export_dictionary_data(df):\n",
    "    df.to_csv(get_output_file('02_data_preprocessing_dictionary.csv'))\n",
    "\n",
    "def export_combined_data(df):\n",
    "    df.to_csv(get_output_file('02_data_preprocessing_final.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing using Name Entity Recognition (NER)\n",
    "<span style=\"color:red\"><b>Important</b>: The NER process takes long processing time (few hours), this is just a one time call and save to a local file, not required for every run.</span>    \n",
    "\n",
    "Use standford NER tagger to extract following information:\n",
    "* Organization\n",
    "* Location\n",
    "* Date\n",
    "* Time\n",
    "\n",
    "Input file: <span style=\"color:blue; font-weight:bold\">01_data_extraction.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_ner.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Setup environment\n",
    "############################################\n",
    "java_path = 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_121\\\\bin\\\\java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "############################################\n",
    "# Define taggers\n",
    "############################################\n",
    "ner7_model_path = os.getcwd() + \"\\\\tools\\\\stanford-ner-2017-06-09\\\\english.muc.7class.distsim.crf.ser.gz\"\n",
    "ner_jar_path = os.getcwd() + \"\\\\tools\\\\stanford-ner-2017-06-09\\\\stanford-ner.jar\"\n",
    "st_ner7 = StanfordNERTagger(ner7_model_path, ner_jar_path)\n",
    "\n",
    "############################################\n",
    "# Import data\n",
    "############################################\n",
    "df = import_data()\n",
    "\n",
    "#############################################################################\n",
    "# # Loop through all rows and extract possible name entities from description\n",
    "#############################################################################\n",
    "count = 1\n",
    "total = len(df.index)\n",
    "date_list = []\n",
    "organization_list = []\n",
    "time_list = []\n",
    "location_list = []\n",
    "\n",
    "def list_name_entities(tagging_result):\n",
    "    \n",
    "    entities = {'DATE': set(), \n",
    "                'ORGANIZATION': set(), \n",
    "                'LOCATION': set(), \n",
    "                'TIME': set()}\n",
    "    \n",
    "    for tag, chunk in groupby(tagging_result, lambda x:x[1]):\n",
    "        if tag in entities.keys():\n",
    "            entity = ' '.join(w.strip() for w, t in chunk)\n",
    "            entities[tag].add(entity)\n",
    "            \n",
    "    for key, value in entities.items():\n",
    "        entities[key] = ', '.join(value)\n",
    "        \n",
    "    return entities\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    sent = row['description']\n",
    "    sent_ner7 = st_ner7.tag(word_tokenize(sent))\n",
    "    entities = list_name_entities(sent_ner7)\n",
    "    \n",
    "    date_list.append(entities['DATE'])\n",
    "    organization_list.append(entities['ORGANIZATION'])\n",
    "    time_list.append(entities['TIME'])\n",
    "    location_list.append(entities['LOCATION'])\n",
    "    \n",
    "    print(f\"Processing row {count} out of {total} with index {index}.\")\n",
    "    count = count + 1\n",
    "\n",
    "############################################\n",
    "# Add new colmumns into original dataframe\n",
    "############################################\n",
    "df['date'] = date_list\n",
    "df['organization'] = organization_list\n",
    "df['time'] = time_list\n",
    "df['location'] = location_list\n",
    "\n",
    "############################################\n",
    "# Export data\n",
    "############################################\n",
    "export_ner_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data preprocessing using dictionary\n",
    "Extract following information using dictionary matching\n",
    "* Occupation\n",
    "* Injured body parts\n",
    "* Is fatal?\n",
    "* Activity\n",
    "\n",
    "Input file: <span style=\"color:blue; font-weight:bold\">01_data_extraction.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_dictionary.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Import data\n",
    "################################\n",
    "df = import_data()\n",
    "\n",
    "################################\n",
    "# Create is_fatal column\n",
    "################################\n",
    "def detect_fatality(case_title):\n",
    "    \n",
    "    def generate_fatality_keywords():\n",
    "        fatality_list = ['death', 'killed', 'dead', 'fatal', 'fatally', 'dies', 'died']\n",
    "        stopword_fatality = ['fall', 'going', 'passing', 'expiration', 'loss', 'exit', 'remove', 'off', 'waste']\n",
    "        final_list = []\n",
    "        final_list = final_list + fatality_list\n",
    "        for fatal_kw in fatality_list:\n",
    "\n",
    "            dead_keywords = wn.synsets(fatal_kw)\n",
    "            dead = wn.synsets(fatal_kw)[0]\n",
    "            keywords = list(set([w for s in dead.closure(lambda s: s.hyponyms()) for w in s.lemma_names()]))\n",
    "            for kw in keywords:\n",
    "                if kw not in final_list and kw not in stopword_fatality:\n",
    "                    final_list.append(kw.replace('_', ' ').lower())\n",
    "        return final_list\n",
    "    \n",
    "    fatality_keyword_list = generate_fatality_keywords()\n",
    "    case_tokens = word_tokenize(str(case_title).lower())\n",
    "    is_fatal = False\n",
    "    \n",
    "    for case_t in case_tokens:\n",
    "        if case_t.strip() in fatality_keyword_list:\n",
    "            is_fatal = True\n",
    "    return is_fatal\n",
    "\n",
    "df['is_fatal'] = df['title'].apply(detect_fatality)\n",
    "\n",
    "################################\n",
    "# Create activity column\n",
    "################################\n",
    "df['activity'] = df['title']\n",
    "\n",
    "################################\n",
    "# Create body_parts column\n",
    "################################\n",
    "stop = stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "dict_body_parts = ['ankle', 'arch', 'arm', 'armpit', 'beard', 'breast', 'calf', 'cheek', 'chest', 'chin', 'earlobe', \n",
    "                   'elbow', 'eyebrow', 'eyelash', 'eyelid', 'face', 'finger', 'forearm', 'forehead', 'gum', 'heel', \n",
    "                   'hip', 'jaw', 'knee', 'knuckle', 'leg', 'lip', 'mouth', 'head']\n",
    "\n",
    "def detect_body_parts(keywords):\n",
    "    tokens = word_tokenize(keywords)\n",
    "    tokens_nop = [t for t in tokens if t not in string.punctuation]\n",
    "    tokens_lower = [t.lower() for t in tokens_nop]\n",
    "    tokens_nostop = [t for t in tokens_lower if t not in stop]\n",
    "    tokens_lem = [wnl.lemmatize(t) for t in tokens_nostop]\n",
    "    body_parts = [t for t in set(tokens_lem) if t in dict_body_parts]\n",
    "    return ', '.join(body_parts)\n",
    "\n",
    "df['body_parts'] = (df['title'] + ' ' + df['description'] + ' ' + df['keywords']).apply(detect_body_parts)\n",
    "\n",
    "################################\n",
    "# Create occupation column\n",
    "################################\n",
    "\n",
    "\n",
    "################################\n",
    "# Export data\n",
    "################################\n",
    "export_dictionary_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combine all features\n",
    "Combine columns from separated files and export to final csv. Additional handling:\n",
    "* Date column will pick the first date in array\n",
    "* Not picking Orgnisation, Person, Location as too many missing values\n",
    "\n",
    "Input files: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_ner.csv, 02_data_preprocessing_dictionary.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing.csv</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Import data\n",
    "################################\n",
    "df_ner = import_ner_data()\n",
    "df_dict = import_dictionary_data()\n",
    "\n",
    "################################\n",
    "# Combine data\n",
    "################################\n",
    "df = pd.DataFrame()\n",
    "df['acitivty'] = df_dict['activity']\n",
    "df['date'] = df_ner['date']\n",
    "df['body_parts'] = df_dict['body_parts']\n",
    "df['is_fatal'] = df_dict['is_fatal']\n",
    "df['occupation'] = None\n",
    "df['topics'] = None\n",
    "\n",
    "################################\n",
    "# Transformation\n",
    "################################\n",
    "\n",
    "def parse_date(dates):\n",
    "    dates = str(dates)\n",
    "    dates = [d.strip() for d in dates.split(',')]\n",
    "    for d in dates:\n",
    "        date = None\n",
    "        try:\n",
    "            date = parse(d)\n",
    "            return date\n",
    "        except ValueError:\n",
    "            continue       \n",
    "    return None\n",
    "\n",
    "df['date'] = df['date'].apply(parse_date)\n",
    "\n",
    "################################\n",
    "# Export data\n",
    "################################\n",
    "export_combined_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
