{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import groupby\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_output_file(filename):\n",
    "    return os.path.join(os.getcwd(), 'output_data', filename)\n",
    "\n",
    "def import_data():\n",
    "    return pd.read_csv(get_output_file('01_data_extraction_final.csv'), index_col=0)\n",
    "\n",
    "def import_ner_data():\n",
    "    return pd.read_csv(get_output_file('02_data_preprocessing_ner.csv'), index_col=0)\n",
    "\n",
    "def import_dictionary_data():\n",
    "    return pd.read_csv(get_output_file('02_data_preprocessing_dictionary.csv'), index_col=0)\n",
    "\n",
    "def import_combined_data():\n",
    "    return pd.read_csv(get_output_file('02_data_preprocessing_final.csv'), index_col=0)\n",
    "\n",
    "def export_ner_data(df):\n",
    "    df.to_csv(get_output_file('02_data_preprocessing_ner.csv'))\n",
    "    \n",
    "def export_dictionary_data(df):\n",
    "    df.to_csv(get_output_file('02_data_preprocessing_dictionary.csv'))\n",
    "\n",
    "def export_combined_data(df):\n",
    "    df.to_csv(get_output_file('02_data_preprocessing_final.csv'))\n",
    "\n",
    "def export_combined_data_ml(df):\n",
    "    df.to_csv(get_output_file('02_data_preprocessing_final_ml.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing using Name Entity Recognition (NER)\n",
    "<span style=\"color:red\"><b>Important</b>: The NER process takes long processing time (few hours), this is just a one time call and save to a local file, not required for every run.</span>    \n",
    "\n",
    "Use standford NER tagger to extract following information:\n",
    "* Organization\n",
    "* Location\n",
    "* Date\n",
    "* Time\n",
    "\n",
    "Input file: <span style=\"color:blue; font-weight:bold\">01_data_extraction.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_ner.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Setup environment\n",
    "############################################\n",
    "java_path = 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_121\\\\bin\\\\java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "############################################\n",
    "# Define taggers\n",
    "############################################\n",
    "ner7_model_path = os.getcwd() + \"\\\\tools\\\\stanford-ner-2017-06-09\\\\english.muc.7class.distsim.crf.ser.gz\"\n",
    "ner_jar_path = os.getcwd() + \"\\\\tools\\\\stanford-ner-2017-06-09\\\\stanford-ner.jar\"\n",
    "st_ner7 = StanfordNERTagger(ner7_model_path, ner_jar_path)\n",
    "\n",
    "############################################\n",
    "# Import data\n",
    "############################################\n",
    "df = import_data()\n",
    "\n",
    "#############################################################################\n",
    "# # Loop through all rows and extract possible name entities from description\n",
    "#############################################################################\n",
    "count = 1\n",
    "total = len(df.index)\n",
    "date_list = []\n",
    "organization_list = []\n",
    "time_list = []\n",
    "location_list = []\n",
    "\n",
    "def list_name_entities(tagging_result):\n",
    "    \n",
    "    entities = {'DATE': set(), \n",
    "                'ORGANIZATION': set(), \n",
    "                'LOCATION': set(), \n",
    "                'TIME': set()}\n",
    "    \n",
    "    for tag, chunk in groupby(tagging_result, lambda x:x[1]):\n",
    "        if tag in entities.keys():\n",
    "            entity = ' '.join(w.strip() for w, t in chunk)\n",
    "            entities[tag].add(entity)\n",
    "            \n",
    "    for key, value in entities.items():\n",
    "        entities[key] = ', '.join(value)\n",
    "        \n",
    "    return entities\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    sent = row['description']\n",
    "    sent_ner7 = st_ner7.tag(word_tokenize(sent))\n",
    "    entities = list_name_entities(sent_ner7)\n",
    "    \n",
    "    date_list.append(entities['DATE'])\n",
    "    organization_list.append(entities['ORGANIZATION'])\n",
    "    time_list.append(entities['TIME'])\n",
    "    location_list.append(entities['LOCATION'])\n",
    "    \n",
    "    print(f\"Processing row {count} out of {total} with index {index}.\")\n",
    "    count = count + 1\n",
    "\n",
    "############################################\n",
    "# Add new colmumns into original dataframe\n",
    "############################################\n",
    "df['date'] = date_list\n",
    "df['organization'] = organization_list\n",
    "df['time'] = time_list\n",
    "df['location'] = location_list\n",
    "\n",
    "############################################\n",
    "# Export data\n",
    "############################################\n",
    "export_ner_data(df)\n",
    "\n",
    "############################################\n",
    "# Inspect data\n",
    "############################################\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data preprocessing using dictionary\n",
    "Extract following information using dictionary matching\n",
    "* Occupation\n",
    "* Injured body parts\n",
    "* Is fatal?\n",
    "* Activity\n",
    "\n",
    "Input file: <span style=\"color:blue; font-weight:bold\">01_data_extraction.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_dictionary.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Import data\n",
    "################################\n",
    "df = import_data()\n",
    "\n",
    "################################\n",
    "# Create is_fatal column\n",
    "################################\n",
    "def detect_fatality(case_title):\n",
    "    \n",
    "    def generate_fatality_keywords():\n",
    "        fatality_list = ['death', 'killed', 'dead', 'fatal', 'fatally', 'dies', 'died']\n",
    "        stopword_fatality = ['fall', 'going', 'passing', 'expiration', 'loss', 'exit', 'remove', 'off', 'waste']\n",
    "        final_list = []\n",
    "        final_list = final_list + fatality_list\n",
    "        for fatal_kw in fatality_list:\n",
    "\n",
    "            dead_keywords = wn.synsets(fatal_kw)\n",
    "            dead = wn.synsets(fatal_kw)[0]\n",
    "            keywords = list(set([w for s in dead.closure(lambda s: s.hyponyms()) for w in s.lemma_names()]))\n",
    "            for kw in keywords:\n",
    "                if kw not in final_list and kw not in stopword_fatality:\n",
    "                    final_list.append(kw.replace('_', ' ').lower())\n",
    "        return final_list\n",
    "    \n",
    "    fatality_keyword_list = generate_fatality_keywords()\n",
    "    case_tokens = word_tokenize(str(case_title).lower())\n",
    "    is_fatal = False\n",
    "    \n",
    "    for case_t in case_tokens:\n",
    "        if case_t.strip() in fatality_keyword_list:\n",
    "            is_fatal = True\n",
    "    return is_fatal\n",
    "\n",
    "df['is_fatal'] = df['title'].apply(detect_fatality)\n",
    "\n",
    "################################\n",
    "# Create activity column\n",
    "################################\n",
    "df['activity'] = df['title']\n",
    "\n",
    "################################\n",
    "# Create body_parts column\n",
    "################################\n",
    "stop = stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "dict_body_parts = ['abdomen','ankle','arch','arm','armpit','back','beard','breast','buttock','calf','cheek',\n",
    "                   'chest','chin','collarbone','ear laceration','earlobe','elbow','eyebrow','eyelash','eyelid',\n",
    "                   'face','femur','finger','forearm','forehead','forehead','groin','gum','hand','head','heel',\n",
    "                   'hip','jaw','knee','knuckle','leg','lip','lungs','mouth','neck','pelvis','ribs','right temple',\n",
    "                   'shoulder','spleen','thigh','throat','thumb','torso','wrist']\n",
    "body_parts_mapping = {\n",
    "    'abdominal fracture': 'abdomen',\n",
    "    'hypertrophic heart disease': 'heart',\n",
    "    'respiratory': 'lungs',\n",
    "    'brain death': 'brain'\n",
    "}\n",
    "\n",
    "def detect_body_parts(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_nop = [t for t in tokens if t not in string.punctuation]\n",
    "    tokens_lower = [t.lower() for t in tokens_nop]\n",
    "    tokens_nostop = [t for t in tokens_lower if t not in stop]\n",
    "    tokens_lem = [wnl.lemmatize(t) for t in tokens_nostop]\n",
    "    body_parts = [t for t in set(tokens_lem) if t in dict_body_parts]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    body_parts = body_parts + [value for key, value in body_parts_mapping.items() if key in text_lower]\n",
    "    return ', '.join(body_parts)\n",
    "\n",
    "df['body_parts'] = (df['title'] + ' ' + df['description'] + ' ' + df['keywords']).apply(detect_body_parts)\n",
    "\n",
    "################################\n",
    "# Create occupation column\n",
    "################################\n",
    "stop = stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "occupations = {\"construction\": [\"construction\"],\n",
    "               \"cleaner\": [\"cleaner\", \"housekeeping\", \"cleaning\"],\n",
    "               \"electrician\": [\"electrician\"],\n",
    "               \"welder\": [\"welder\", \"welding\"],\n",
    "               \"farmer\": [\"agriculture\", \"farm\", \"pruner\"],\n",
    "               \"firefighter\": [\"firefighter\"],\n",
    "               \"operator\": [\"operator\"],\n",
    "               \"plumber\": [\"plumber\", \"plumbing\"],\n",
    "               \"painter\": [\"painter\"],\n",
    "               \"Smelter workers\": [\"molten\"],\n",
    "               \"driver\": [\"driver\"],\n",
    "               \"logger\": [\"logging\"],\n",
    "               \"roofer\": [\"roofer\"],\n",
    "               \"machinist\": [\"machinist\"],\n",
    "               }\n",
    "\n",
    "def detect_occupation(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_nop = [t for t in tokens if t not in string.punctuation]\n",
    "    tokens_lower = [t.lower() for t in tokens_nop]\n",
    "    tokens_nostop = [t for t in tokens_lower if t not in stop]\n",
    "    tokens_lem_noun = [wnl.lemmatize(t) for t in tokens_nostop]\n",
    "    tokens_lem_verb = [wnl.lemmatize(t, pos='v') for t in tokens_lem_noun]\n",
    "\n",
    "    occupation = set()\n",
    "    for t in tokens_lem_verb:\n",
    "        for key in occupations.keys():\n",
    "            if (t in occupations[key]):\n",
    "                occupation.add(key)\n",
    "\n",
    "    return ', '.join(occupation)\n",
    "\n",
    "df['occupation'] = (df['title'] + ' ' + df['description'] + ' ' + df['keywords']).apply(detect_occupation)\n",
    "\n",
    "################################\n",
    "# Export data\n",
    "################################\n",
    "export_dictionary_data(df)\n",
    "\n",
    "################################\n",
    "# Inspect data\n",
    "################################\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combine all features\n",
    "Combine columns from separated files and export to final csv. Additional handling:\n",
    "* Date column will pick the first date in array\n",
    "* Not picking Orgnisation, Person, Location as too many missing values\n",
    "\n",
    "Input files: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_ner.csv, 02_data_preprocessing_dictionary.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_final.csv, 02_data_preprocessing_final_ml.csv</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Import data\n",
    "################################\n",
    "df = import_data()\n",
    "df_ner = import_ner_data()\n",
    "df_dict = import_dictionary_data()\n",
    "\n",
    "################################\n",
    "# Combine data\n",
    "################################\n",
    "df['acitivty'] = df_dict['activity']\n",
    "df['date'] = df_ner['date']\n",
    "df['body_parts'] = df_dict['body_parts']\n",
    "df['occupation'] = df_dict['occupation']\n",
    "df['is_fatal'] = df_dict['is_fatal']\n",
    "df['topics'] = None\n",
    "\n",
    "################################\n",
    "# Transformation\n",
    "################################\n",
    "\n",
    "def parse_date(dates):\n",
    "    dates = str(dates)\n",
    "    dates = [d.strip() for d in dates.split(',')]\n",
    "    for d in dates:\n",
    "        date = None\n",
    "        try:\n",
    "            date = parse(d)\n",
    "            return date\n",
    "        except ValueError:\n",
    "            continue       \n",
    "    return None\n",
    "\n",
    "df['date'] = df['date'].apply(parse_date)\n",
    "\n",
    "################################\n",
    "# Export data\n",
    "################################\n",
    "export_combined_data(df)\n",
    "\n",
    "################################\n",
    "# Inspect data\n",
    "################################\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Import data\n",
    "################################\n",
    "df = import_combined_data()\n",
    "\n",
    "############################################################\n",
    "# Convert column with array values to separated indicators\n",
    "############################################################\n",
    "\n",
    "def array_to_indicators(df, col, prefix):\n",
    "    labels = set()\n",
    "    for values in df[col]:\n",
    "        for value in str(values).split(','):\n",
    "            labels.add(value.strip())\n",
    "\n",
    "    for label in labels:\n",
    "        df[f'{prefix}_{label}'] = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        values = str(row[col]).split(',')\n",
    "        labels = [value.strip() for value in values]\n",
    "        for label in labels:\n",
    "            df.loc[index, f'{prefix}_{label}'] = 1\n",
    "\n",
    "    del df[col]\n",
    "    return df\n",
    "    \n",
    "df = array_to_indicators(df, 'body_parts', 'body')\n",
    "\n",
    "################################\n",
    "# Export data\n",
    "################################\n",
    "export_combined_data_ml(df)\n",
    "\n",
    "################################\n",
    "# Inspect data\n",
    "################################\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
