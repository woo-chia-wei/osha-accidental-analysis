{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    filename = os.getcwd() + '\\\\output_data\\\\01_data_extraction.csv'\n",
    "    return pd.read_csv(filename, index_col=0)\n",
    "\n",
    "def export_ner_data(df):\n",
    "    filename = os.getcwd() + '\\\\output_data\\\\02_data_preprocessing_ner.csv'\n",
    "    df.to_csv(filename)\n",
    "    \n",
    "def export_grammar_data(df):\n",
    "    filename = os.getcwd() + '\\\\output_data\\\\02_data_preprocessing_grammar.csv'\n",
    "    df.to_csv(filename)\n",
    "    \n",
    "def export_keywords_data(df):\n",
    "    filename = os.getcwd() + '\\\\output_data\\\\02_data_preprocessing_keywords.csv'\n",
    "    df.to_csv(filename)\n",
    "\n",
    "def export_combined_data(df):\n",
    "    filename = os.getcwd() + '\\\\output_data\\\\02_data_preprocessing.csv'\n",
    "    df.to_csv(filename)\n",
    "    \n",
    "def list_name_entities(tagging_result):\n",
    "    \n",
    "    entities = {'DATE': set(), \n",
    "                'ORGANIZATION': set(), \n",
    "                'LOCATION': set(), \n",
    "                'TIME': set()}\n",
    "    \n",
    "    for tag, chunk in groupby(tagging_result, lambda x:x[1]):\n",
    "        if tag in entities.keys():\n",
    "            entity = ' '.join(w.strip() for w, t in chunk)\n",
    "            entities[tag].add(entity)\n",
    "            \n",
    "    for key, value in entities.items():\n",
    "        entities[key] = ', '.join(value)\n",
    "        \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing using Name Entity Recognition (NER)\n",
    "<span style=\"color:red\"><b>Important</b>: The NER process takes long processing time (few hours), this is just a one time call and save to a local file, not required for every run.</span>    \n",
    "\n",
    "Use standford NER tagger to extract following information:\n",
    "* Organization\n",
    "* Location\n",
    "* Date\n",
    "* Time\n",
    "\n",
    "Input file: <span style=\"color:blue; font-weight:bold\">01_data_extraction.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_ner.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 1 out of 20 with index 202561825.\n",
      "Processing row 2 out of 20 with index 200361855.\n",
      "Processing row 3 out of 20 with index 200361863.\n",
      "Processing row 4 out of 20 with index 201079324.\n",
      "Processing row 5 out of 20 with index 202658258.\n",
      "Processing row 6 out of 20 with index 202685947.\n",
      "Processing row 7 out of 20 with index 202673471.\n",
      "Processing row 8 out of 20 with index 202369575.\n",
      "Processing row 9 out of 20 with index 202509832.\n",
      "Processing row 10 out of 20 with index 201129681.\n",
      "Processing row 11 out of 20 with index 202081899.\n",
      "Processing row 12 out of 20 with index 202082020.\n",
      "Processing row 13 out of 20 with index 201562840.\n",
      "Processing row 14 out of 20 with index 202674610.\n",
      "Processing row 15 out of 20 with index 202457990.\n",
      "Processing row 16 out of 20 with index 201510823.\n",
      "Processing row 17 out of 20 with index 202478632.\n",
      "Processing row 18 out of 20 with index 200361178.\n",
      "Processing row 19 out of 20 with index 202692364.\n",
      "Processing row 20 out of 20 with index 202615324.\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "java_path = 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_121\\\\bin\\\\java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "# Define taggers\n",
    "ner7_model_path = os.getcwd() + \"\\\\tools\\\\stanford-ner-2017-06-09\\\\english.muc.7class.distsim.crf.ser.gz\"\n",
    "ner_jar_path = os.getcwd() + \"\\\\tools\\\\stanford-ner-2017-06-09\\\\stanford-ner.jar\"\n",
    "st_ner7 = StanfordNERTagger(ner7_model_path, ner_jar_path)\n",
    "\n",
    "# Import data\n",
    "df = import_data()\n",
    "df = df[:20] # Try with small set of data\n",
    "\n",
    "# Loop through all rows\n",
    "count = 1\n",
    "total = len(df.index)\n",
    "date_list = []\n",
    "organization_list = []\n",
    "time_list = []\n",
    "location_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    sent = row['description']\n",
    "    sent_ner7 = st_ner7.tag(word_tokenize(sent))\n",
    "    entities = list_name_entities(sent_ner7)\n",
    "    \n",
    "    date_list.append(entities['DATE'])\n",
    "    organization_list.append(entities['ORGANIZATION'])\n",
    "    time_list.append(entities['TIME'])\n",
    "    location_list.append(entities['LOCATION'])\n",
    "    \n",
    "    print(f\"Processing row {count} out of {total} with index {index}.\")\n",
    "    count = count + 1\n",
    "\n",
    "# Add new colmumns into original dataframe\n",
    "df['date'] = date_list\n",
    "df['organization'] = organization_list\n",
    "df['time'] = time_list\n",
    "df['location'] = location_list\n",
    "\n",
    "# Export data\n",
    "export_ner_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data preprocessing using grammar parser\n",
    "Use nltk grammar parser to extract following information:\n",
    "* Activity\n",
    "* Is fatal?\n",
    "\n",
    "Input file: <span style=\"color:blue; font-weight:bold\">01_data_extraction.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_grammar.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = import_data()\n",
    "\n",
    "# Write your steps here...\n",
    "\n",
    "# Export data\n",
    "export_grammar_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data preprocessing using taxonomy matching\n",
    "Apply taxonomy matching to extract following information from keywords\n",
    "* Occupation\n",
    "* Injured body parts\n",
    "\n",
    "Input file: <span style=\"color:blue; font-weight:bold\">01_data_extraction.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_keywords.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = import_data()\n",
    "\n",
    "# Write your steps here...\n",
    "\n",
    "# Export data\n",
    "export_keywords_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combine all features\n",
    "Combine columns from separated files and export to final csv.\n",
    "\n",
    "Input files: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing_ner.csv, 02_data_preprocessing_grammar.csv, 02_data_preprocessing_keywords.csv</span>  \n",
    "Output file: <span style=\"color:blue; font-weight:bold\">02_data_preprocessing.csv</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
