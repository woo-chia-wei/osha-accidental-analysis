{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag, ne_chunk,FreqDist,sent_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import groupby\n",
    "import gensim\n",
    "from gensim import corpora,models\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read in Input Files from the Preprocessing Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file = open('C:\\\\NUS_ISS\\\\KE5205_Text_Mining\\\\Assignment\\\\02_data_preprocessing_final.csv','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Create a list of different categories of customised stopwords and combine it together with NLTK's english stopword list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_stopwords = ['p','m','january','february','march','april','may','june','july','august','september','october','november','december']\n",
    "custom_stopwords = [line.strip() for line in open('custom_stopwords.txt','r')]\n",
    "entity_stopwords = ['hospital','employee','employer','worker','coworker','supervisor']\n",
    "stopset =stopwords.words('english') + temporal_stopwords + custom_stopwords + entity_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data,stopword_lis):\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    data=data.strip()\n",
    "    #remove punct\n",
    "    text_no_punct = ' '.join(word.strip(string.punctuation) for word in data.split())\n",
    "\n",
    "    #tokenize\n",
    "    tokens = nltk.regexp_tokenize(data.lower(), pattern='\\w+')\n",
    "    stop_word_remove = [token.lower() for token in tokens if token.lower().strip() not in stopword_lis]\n",
    "    \n",
    "    #remove digits\n",
    "    no_digit_no_stopwords = [tok for tok in stop_word_remove if not tok.isdigit()]\n",
    "    result_post_lemma = [wnl.lemmatize(t) for t in no_digit_no_stopwords]\n",
    "    remove_stop_words_post_lemma = [token.lower() for token in result_post_lemma if token.lower().strip() not in stopword_lis]\n",
    "\n",
    "    return remove_stop_words_post_lemma\n",
    "\n",
    "def retag_documents(work_sheet,model_file,dictionary_ref,topic_list_off,filename):\n",
    "\n",
    "    output_file = open(filename,'w')\n",
    "    output_file.write('case_id,title,description,keywords,victims,activity,date,body_part,occupation,is_fatal,topics,topic_desc\\n')\n",
    "    count = 0\n",
    "    for rowz in work_sheet:\n",
    "        rowz_value = rowz.split(',')\n",
    "        if count > 0:\n",
    "            id_tag = rowz_value[0]\n",
    "            title_tag = rowz_value[1].lower().strip()\n",
    "            summary_tag = rowz_value[2].lower().strip()\n",
    "            metatag = rowz_value[3].lower()\n",
    "            text_list_tag = preprocess(summary_tag, stopset)\n",
    "            result_doc_topics = model_file.get_document_topics(dictionary_ref.doc2bow(text_list_tag))\n",
    "            #assign only the highest probablity topic \n",
    "            result_data_max = max(result_doc_topics, key=itemgetter(1))\n",
    "            output_file.write(rowz.replace('\\n', '')+','+str(result_data_max[0])+','+topic_list_off[result_data_max[0]]+'\\n')\n",
    "        count=count+1\n",
    "    output_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Topic Modeling Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_document_list(input_file):\n",
    "    doc_list = []\n",
    "    count_title = 0\n",
    "    for row in csv_file:\n",
    "        row_value = row.split(',')\n",
    "        if count_title > 0:\n",
    "            id = row_value[0]\n",
    "            title = row_value[1].lower()\n",
    "            summary = row_value[2].lower()\n",
    "            metatag = row_value[3].lower()\n",
    "            if title.lower().strip() != 'inspectionopen datesicestablishment name':\n",
    "                text_list = preprocess(str(summary), stopset)\n",
    "                if len(text_list) > 0:\n",
    "                    doc_list.append(text_list)\n",
    "        count_title = count_title + 1\n",
    "    return doc_list\n",
    "\n",
    "def create_dictionary(docu_list):\n",
    "    dictionary = corpora.Dictionary(docu_list)\n",
    "    return dictionary\n",
    "\n",
    "def create_dictionary_bow(dict_list,docs_list):\n",
    "    print(dict_list)\n",
    "    dict_list.filter_extremes(no_below=15, no_above=0.85)\n",
    "    dtm_train = [dict_list.doc2bow(d) for d in docs_list]\n",
    "    return dtm_train\n",
    "\n",
    "def create_topic_model(training_rep,dictionary):\n",
    "\n",
    "    ldamodel1 = models.ldamodel.LdaModel(training_rep, num_topics=15, id2word=dictionary, passes=20)\n",
    "    ldamodel1.save('lda_model1.mod')\n",
    "    return ldamodel1\n",
    "\n",
    "def clean_topic_list(topic_list_raw):\n",
    "    topic_list_official = []\n",
    "    for topics, topic_desc in topic_list_raw:\n",
    "        topic_post_clean = nltk.regexp_tokenize(topic_desc.lower(), pattern='\\w+')\n",
    "        result = '+'.join([w for w in topic_post_clean if not w.isdigit()])\n",
    "        print(str(topics)+' '+str(result))\n",
    "        topic_list_official.append(result)\n",
    "    return topic_list_official"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Topic Modelling Process - Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(22390 unique tokens: ['edco', 'waste', 'recycling', 'service', 'operating']...)\n",
      "0 work+investigation+time+incident+safety+stated+training+equipment\n",
      "1 power+line+electrical+electric+energized+conductor+volt+electrocuted\n",
      "2 degree+burn+water+second+hot+trench+third+arm\n",
      "3 emergency+fire+room+transported+service+area+called+later\n",
      "4 tree+crane+operator+struck+beam+load+fell+steel\n",
      "5 pipe+air+pressure+valve+line+well+pump+water\n",
      "6 line+lift+pole+cable+ground+rope+wire+tower\n",
      "7 conveyor+machine+roller+caught+belt+arm+bin+hand\n",
      "8 machine+finger+hand+press+number+operating+hospitalized+operator\n",
      "9 wall+floor+foot+concrete+car+side+fell+rail\n",
      "10 tank+fire+gas+explosion+burn+chemical+hospitalized+vapor\n",
      "11 truck+forklift+trailer+vehicle+tire+side+tractor+struck\n",
      "12 sheet+board+construction+work+gun+plywood+using+new\n",
      "13 fell+foot+ladder+roof+hospitalized+fall+injury+fracture\n",
      "14 cut+saw+piece+metal+blade+hand+cutting+inch\n"
     ]
    }
   ],
   "source": [
    "topic_output_filename='ocha_data_topics.csv'\n",
    "\n",
    "# Create Document List\n",
    "\n",
    "document_list = create_document_list(csv_file)\n",
    "\n",
    "#Create Word to ID mappings \n",
    "dictionary_list = create_dictionary(document_list)\n",
    "\n",
    "#Create Bag of Words \n",
    "dict_bow = create_dictionary_bow(dictionary_list,document_list)\n",
    "\n",
    "#Create Topic Model\n",
    "topic_model = create_topic_model(dict_bow,dictionary_list)\n",
    "topic_list_extracted = topic_model.show_topics(num_topics=15, num_words=8)\n",
    "\n",
    "topic_list_clean = clean_topic_list(topic_list_extracted)\n",
    "csv_file2 = open('C:\\\\NUS_ISS\\\\KE5205_Text_Mining\\\\Assignment\\\\02_data_preprocessing_final.csv','r')\n",
    "\n",
    "# Tag each ocha report to a topic\n",
    "retag_documents(csv_file2,topic_model,dictionary_list,topic_list_clean,topic_output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.50962415077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lda_coherence = [n for _, n in topic_model.top_topics(dict_bow)]\n",
    "print(np.mean(lda_coherence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the documents in the ocha report has been tagged with individual topics, we will manually examine the generated csv file to gain an understanding of the topics and try to discover underlying causes. We will attempt to derive possible causes of accidents through the topics identified. This can be a highly iterative and subjective process especially finding the best n topics for the ocha data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
